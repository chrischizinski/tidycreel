Estimating Fishing Effort in Creel Surveys

Introduction: Recreational angler (creel) surveys are on-site sampling programs used to estimate fishing effort (often measured in angler-hours or angler-trips) and catch. Creel survey designs combine angler counts (to gauge fishing pressure) with angler interviews (to gather catch data), and expand these observations to estimate total effort and catch over a period ￼. Estimating total fishing effort requires statistically sound methods to extrapolate from sampled observations (e.g. counts at certain times or locations) to the entire fishing day or season. Below we review both traditional fisheries-specific survey techniques and general survey-sampling estimators used for creel surveys, as well as emerging model-based approaches. For each, we summarize core principles, data requirements, pros/cons, and any R implementations (in packages like survey, srvyr, or specialized fisheries packages).

Fisheries-Specific Creel Survey Techniques for Effort

Instantaneous Count Surveys

Principle: An instantaneous count is a snapshot tally of all anglers actively fishing at a given moment in time across the survey area. By selecting count times at random (or otherwise representative) moments, managers assume the instantaneous count reflects the average number of anglers present. To estimate total effort for a day, the typical approach is to multiply the mean number of anglers observed by the length of the fishing day (in hours) ￼. This essentially integrates the instantaneous angler count over time, under the assumption that anglers’ start and end times are randomly distributed. Instantaneous counts can be obtained by a creel clerk from a vantage point (e.g. a hill or pier) or via aerial overflight, such that the entire fishing area is covered nearly simultaneously.

Data Requirements: Requires the ability to observe all (or nearly all) anglers at once. This may involve a single observer if the waterbody is small, or multiple observers or aerial photography for larger areas. Each count is paired with the exact time. Often multiple instantaneous counts are taken at randomly chosen times of day and on randomly sampled days to account for diurnal and daily variation in use.

Pros: The method is conceptually simple and, when properly randomized, provides an unbiased estimator of total angler-hours ￼. It does not require tracking individual anglers or interviewing them, focusing only on counts. Instantaneous counts are especially efficient with aerial surveys (see below), where a quick flyover can count anglers on large areas in minutes.

Cons: Instantaneous counts may miss temporal variation if only a few snapshots are taken; if fishing effort peaks at certain times (morning/evening), a small number of random counts can have high variance. In practice, some agencies historically took counts at fixed times (e.g. noon) for convenience, but non-random timing can introduce bias if angler activity is not uniform ￼. Additionally, an instantaneous count is only feasible if observers can see all fishing activity at once – which can be challenging on complex or large waterbodies without aerial support. Finally, this method provides total effort but no direct information on catch (catch must be obtained via separate interviews).

R Implementations: Instantaneous count expansion is straightforward to implement with base R or survey design tools. One would typically compute the mean count per instant and multiply by day length (with stratification by day-type or month as needed). The survey package can be used to incorporate the sampling design – for example, treating each count as a sampling unit with a weight representing the fraction of the day it represents. However, for simple designs a direct calculation is often used. No specialized R function is required beyond standard summary statistics, though custom scripts or the creelr package (see below) could handle this as part of a larger analysis workflow.

Progressive Count (Roving) Surveys

Principle: A progressive count (also called a roving count) involves an agent physically moving through the fishing area (by boat or vehicle or on foot) and counting anglers continuously or sequentially along a route ￼ ￼. In essence, the clerk “roves” the area and notes each angler encountered. A full progressive count consists of completing a circuit that covers the entire fishing area. Because this takes time, the count is not truly instantaneous – some anglers might start or finish fishing while the clerk is en route. To produce an effort estimate, the count is treated as if it were an “extended instant” covering the pass time. The total count is multiplied by the ratio of the full day length to the time it took to complete the route (the number of such route-length intervals in a day) ￼. Under certain conditions – notably if the start time of the route is random and the clerk moves at constant speed – the progressive count estimator is unbiased and effectively equivalent to an instantaneous count covering the same area ￼. In fact, in an ideal scenario a complete progressive count yields the same total effort estimate as an instantaneous count, just scaled for the time interval of the route ￼.

Data Requirements: The clerk needs to traverse a pre-defined route that covers all fishing locations. The start time of the route should be randomized (and direction may be randomized if the route can be done in reverse) to avoid always counting certain locations at the same time of day ￼. The duration of the route (time to make one full pass) must be measured. The count of anglers observed is recorded, often by segment or continuously, but ultimately a total for the pass is used.

Pros: Progressive counts allow coverage of large or linear areas (e.g. long rivers or big lake shorelines) where a true instantaneous count is impractical. By moving through the area, a single clerk can sample a broad area without aerial support. When properly implemented (random start, consistent speed), it provides an unbiased effort estimate much like instantaneous counts ￼. It also yields spatial information on where anglers are encountered along the route.

Cons: The main challenge is that the clerk is not everywhere at once. If anglers’ movements or fishing times are correlated with the clerk’s schedule, bias can occur. For example, if many anglers tend to leave just before the clerk arrives on a typical circuit, the counts will underestimate true effort. Bias is a known issue if the clerk also conducts interviews during the count: stopping to interview anglers means some anglers arriving or leaving during the pause might not be counted, leading to undercounts ￼. Studies found that combining interviews with progressive counts can underestimate effort because anglers are missed while the clerk is occupied ￼. To mitigate this, some designs impose “no-interview” periods during the counting pass or use multiple clerks (one counts, one interviews). The progressive count also requires assumption of constant travel speed and a closed population of anglers during the short interval of the count (no one is counted twice).

R Implementations: As with instantaneous counts, progressive counts are typically expanded by a simple multiplier (day length / route duration). No off-the-shelf R function is dedicated to this calculation, but it is easily done in base R. One must be careful to propagate variance if using sample data (e.g. if only a subset of days have progressive counts). The survey package can again accommodate this by treating each day or count as a sampled cluster. The variance formulas for progressive count estimators have been derived in the literature (e.g. see Hoenig et al. 1993 for requirements and variance of roving counts), but one would implement those manually or via resampling methods. In practice, agencies often use custom spreadsheets or scripts for these calculations; incorporating them into tidycreel would involve coding these expansion formulas.

Roving Creel Surveys (Interviews on the Go)

Principle: A roving creel survey generally refers to a design where the clerk moves through the fishery to interview anglers during their fishing trips, rather than only at the end (access point). In terms of effort estimation, roving designs typically rely on counts (instantaneous or progressive, as above) to estimate total effort, because anglers are encountered before their trip is over – making it hard to know their total fishing time from that encounter. The roving survey’s hallmark is that the clerk roves a route and may interview anglers about their fishing (catch, start time, etc.) as they are encountered. Effort can be estimated by counting the number of anglers actively fishing (using either instantaneous snapshots or continuous progressive counts along the route). If using a progressive count with interviews, careful design (such as scheduled “count-only” periods or predetermined stop times) is needed to avoid bias from interrupted counts ￼.

Data Requirements: A schedule of travel through the fishery (similar to the progressive count route) is needed. The clerk typically records all anglers seen (for count) and may interview a sample of them for catch or trip information. Key data for effort are the counts of anglers observed and the fraction of the fishing period covered. Often roving surveys stratify the day into time blocks (morning, afternoon) and randomly select which blocks to patrol.

Pros: Roving surveys are well-suited to rivers, large shorelines, or areas with multiple access points where anglers can start/stop anywhere. They allow the collection of catch rate data in addition to counts, since the clerk can ask anglers about catches or harvest during the trip. In sparsely populated fisheries, a roving clerk can find anglers that might not be captured at a single access point. Effort estimates from roving counts are design-based and do not require anglers to report their own effort (unlike off-site surveys).

Cons: As mentioned, if the clerk is multitasking (counting and interviewing), effort estimates can be biased low ￼. Another issue is incomplete trip data: interviews taken during the trip mean the clerk doesn’t observe full trip lengths, so effort must rely on counts rather than summed interview data. Catch-per-unit-effort estimates from roving surveys need special formulas to account for the bias of interviewing mid-trip (longer trips are more likely to be sampled) – though this pertains to catch rates more than total effort. Roving surveys also require more travel time and can be logistically complex. The variance of effort estimates can be higher if fishing activity is patchy in time or space and not all areas are covered at once.

R Implementations: There isn’t a single R package for roving survey estimation, but one would use a combination of tools. The survey package can be employed to treat the complex design (days as clusters, possibly time blocks as sub-strata). One could use svymean to estimate average anglers per count and multiply by hours, or svytotal to directly estimate total angler-hours over the season by assigning an appropriate weight to each observed angler or count. Custom code is often needed for the bias-correction of catch rates (Malvestuto 1978 formulas), but that’s beyond just effort. Some fisheries agencies have their own scripts; for example, the creelr prototype package implements a “traditional access” design, but for roving designs no turnkey package is widely available yet (one goal of tidycreel may be to fill this gap).

Bus-Route Access Point Surveys

Principle: The bus-route survey is a variant of an access-point creel survey designed for large geographic areas with multiple access sites ￼ ￼. In a traditional access-point design, a clerk stays at a boat ramp or access site and interviews all (or a sample of) anglers as they complete their trips. The bus-route method extends this by having a single clerk rotate among several access sites on a fixed schedule – analogous to a bus making stops on a route ￼. The clerk travels between sites according to predetermined arrival and departure times, and while at each site records the presence of fishing party vehicles or conducts interviews for completed trips. The key innovation for effort estimation is to use the observed parking duration of vehicles (or the fact of a vehicle being present at a given time window) to estimate total effort. Robson and Jones (1989) developed an estimator for total angler effort (in party-hours or angler-hours) based on the probability of encountering each fishing party during these scheduled visits ￼. Essentially, if a fishing trip lasts longer, it is more likely the clerk will “catch” that party’s vehicle in a periodic visit. By accounting for the timing and duration of visits, one can derive an unbiased estimate of total effort given certain design conditions.

Data Requirements: A fixed schedule of site visits (including travel times and waiting times at each site) is established. At each site visit, the clerk might record each vehicle’s arrival or departure times if possible, or more practically, note which vehicles (or boat trailers) are present upon arrival and still present at departure. These observations allow the inference of partial trip durations (e.g., “car was here for the entire 30-minute visit”). Importantly, for unbiased estimation the cycle time (travel + wait at a site) should be the same for all sites on the route ￼ ￼. If each site gets equal coverage in that sense, then each fishing trip has an equal chance of being “caught” starting from one of the sites (no site is always visited first or last). The total effort estimator uses the fraction of time a party was observed to extrapolate to total fishing time. In practice, agencies often use formulas from Robson & Jones (1989) or software to compute effort from these observations.

Pros: The bus-route design is very cost-effective for covering many access points with fewer staff ￼. Instead of staffing each ramp all day, one clerk can cover many sites and still estimate total effort. This method is valuable for large lakes or bays with numerous entry points. When properly designed (meeting the equal cycle-time condition for unbiasedness ￼), it yields reliable effort estimates. Studies have shown that bus-route estimators can perform comparably to aerial counts in large systems ￼. Another advantage is safety/feasibility: if night fishing is common (when aerial counts are impossible), a bus-route clerk can operate at night by driving to ramps.

Cons: Designing the schedule is complex – the travel and wait times must be balanced. If the equal cycle-time condition is violated, the estimator can be biased ￼ ￼. For example, if one site is always the starting point (and thus effectively gets more coverage at the expense of another site that’s always visited last), effort may be underestimated or overestimated at some sites ￼. Additionally, bus-route surveys typically measure effort in terms of angler-trips or party-hours rather than direct angler counts, so careful bookkeeping of vehicle/party observations is needed. Field logistics (driving long distances, sticking to schedule despite traffic or weather) can be challenging. Because each site is only observed intermittently, very short fishing trips might be missed entirely (if a party comes and goes between clerk visits). This introduces a slight negative bias, usually small if trips are not extremely short. Variance can be higher than a continuous access survey because of the intermittent observation.

R Implementations: There is no built-in base R function for the bus-route estimator, but the formula from Robson & Jones (1989) can be implemented. The AnglerCreelSurveySimulation package (Ranney 2021) on CRAN provides functions to simulate bus-route surveys and even computes the Robson-Jones effort estimate (notated as Ê or Eb) ￼ ￼. This simulation package cites the original references and can be instructive. For real data analysis, one could use the survey package by treating each observed fishing party with a weight inversely related to its detection probability (a form of Horvitz-Thompson weighting). In fact, the bus-route estimator can be derived from inclusion probabilities of trips being observed ￼. Practically, custom code or spreadsheets are often used by agencies; incorporating this into an R package like tidycreel would involve coding the estimator and variance formula (Jones et al. 1990 describe a variance estimator via repeated surveys ￼). The literature (e.g., Chen & Woolcock 1999) provides guidance on design and analysis for unbiased bus-route surveys ￼ ￼.

Aerial Count Surveys

Principle: Aerial surveys involve using an aircraft (or drone in modern variants) to count anglers or boats on the water from above. An aerial count is essentially an instantaneous count covering a large area quickly ￼. A plane can fly over a lake or river at a set time and observers (or cameras) count all visible anglers, shore fishing parties, or boating anglers. These counts, when scheduled properly (usually at random times within strata like weekends vs weekdays), provide an estimate of the instantaneous fishing effort. Like ground-based instantaneous counts, aerial counts are expanded by the proportion of the day they represent – often by multiplying the average count by the total hours in the day or total days in the season (with stratification as needed). In many designs, aerial counts are paired with on-site access interviews (aerial-access design): the aerial component delivers total effort, and ground crews at access points sample anglers for catch and trip information ￼.

Data Requirements: Requires access to an aircraft and good visibility of the fishing area. The flight plan and timing must be designed carefully. Typically, a flight will cover multiple lakes or a long river segment in one circuit. The timing of flights is randomized or rotated so that all parts of the day are represented across the survey period. Data recorded are the counts of anglers (or boats) observed, often by sub-area if the lake is large (to allow post-stratification or to ensure full coverage). If visibility is an issue (e.g., glare, weather, dense cover), observers may also record an estimate of what fraction of the area was observable; methods exist to correct for visibility bias ￼ (e.g., Pollock and Kendall 1987 discuss sightability adjustments for aerial wildlife counts, which can be applied to anglers).

Pros: Aerial counts can cover very large areas in a short time, enabling essentially instantaneous counts on big waterbodies or across many lakes. This makes it possible to estimate effort for fisheries that would be infeasible to cover with a single ground clerk at one time. Aerial methods are widely used for big reservoirs and Great Lakes fisheries. Because the plane can also cover remote or inaccessible areas, this method is ideal for fisheries where anglers are spread out. When combined with access interviews, the aerial-access design is considered a gold standard for large lakes – the aerial count yields total effort, and interviewers stationed at ramps (possibly only during part of the day) collect catch rates. Aerial counts, if truly random in timing, are a design-unbiased estimator of effort just like ground counts. Empirical tests have found that a well-designed instantaneous count survey (including aerial counts) can closely approximate a full census of effort ￼.

Cons: The cost of aerial surveys is high (fuel, aircraft rental, personnel), so sample sizes (number of flights) may be limited. They are also limited to daylight hours and decent weather – for example, night fishing effort or heavy rain may be missed or require separate estimation. Visibility bias is a concern: not all anglers may be visible from the air (e.g., under tree canopies or in camouflaged boats). Double-counting can happen if the plane circles or if anglers move between sectors during a flight, though with a proper flight path this is minimized. Safety and logistics mean flights are often scheduled at set times that avoid early morning or late evening (which could bias against peak fishing times at dawn/dusk unless adjusted). Finally, aerial counts typically identify number of anglers or boats, but not details of catch or species – thus they must be paired with ground surveys or other methods for a complete picture.

R Implementations: There isn’t a special R package for aerial survey analysis, but the data can be handled like any other count survey. One would use the same expansion formula (mean count * hours * days, etc., with stratification). The survey package can incorporate aerial count data by specifying, for example, each flight as a random sample of the fishing day. If flights are stratified by month or day-type, design weights = (total hours in stratum)/(number of flights in stratum) can be used for a Horvitz-Thompson estimate of total effort. Some fisheries-specific software (often internal) exists for aerial-access designs; for instance, government analysts sometimes use custom Excel or SAS programs. In R, one could also model aerial counts using Poisson regression or mixed models (treating time-of-day effects) – veering into model-based approaches (discussed later). Overall, analyzing aerial counts in R largely means using standard survey expansions or calibrating for sightability if data (e.g., comparison with ground counts) allow; these tasks could be codified in the planned tidycreel package.

Aside: Off-Site Surveys (telephone, mail, or license-based surveys) can also estimate effort by asking anglers how often or how long they fished. While not the focus here, these methods rely on general survey sampling estimators or self-reports rather than on-site counts. They have their own biases (memory, nonresponse) but can complement on-site creels especially for large-scale effort estimation.

Design-Based Survey Sampling Methods for Creel Data

Traditional creel survey analyses lean heavily on survey sampling theory – treating the collected data as samples from a larger population of fishing trips or fishing days. The goal is often to obtain an unbiased estimator for total effort with an estimate of variance, under the random sampling design that was used. Key statistical methods include:
	•	Stratified and Multi-Stage Sampling Estimation: Creel surveys usually stratify effort by time periods (e.g. month, weekend vs weekday) to improve precision. For instance, a common design is stratified random sampling of days (with weekend-days as one stratum and weekdays as another, given different fishing pressure). Within each sampled day, an intra-day sample (such as a random count time or a random subset of access sites) might be taken – this is a two-stage sample. The estimator for total effort in a stratified design is the sum of stratum estimates: e.g., $\hat{E} = \sum_h N_h \bar{e}_h$, where $N_h$ is total days in stratum $h$ and $\bar{e}_h$ is mean effort per day from sampled days in that stratum. This is a standard expansion: average expansion (“mean-of-ratios”) approach. If each day is fully observed (like a complete count or all trips counted), then $\bar{e}_h$ is just the sample mean daily effort. If days are partially observed (e.g. one count per day), that gets folded into the daily estimate calculation. Stratification helps ensure estimates reflect known differences (typically weekends have more anglers than weekdays, etc.). Multi-stage variance formulas (e.g., day-to-day variation and within-day variation) are used to compute confidence intervals ￼. These formulas can be complex, so analysts often use the survey sampling toolkit or replicate methods (jackknife/bootstrapping).
	•	Horvitz-Thompson (H-T) Estimators: The Horvitz-Thompson estimator is the foundation of design-based inference for totals. It states that $\hat{T} = \sum_{i \in S} \frac{y_i}{\pi_i}$ is an unbiased estimator of the population total $T=\sum_{i} y_i$, where $\pi_i$ is the inclusion probability of unit $i$ in the sample. In creel context, a “unit” could be an angler-trip or a time interval. For example, in an access survey where a fraction of angler-trips are sampled, each interviewed trip can be weighted by the inverse of its selection probability to estimate total trips. In a bus-route survey, each fishing trip has some probability of being encountered by the route; the Robson-Jones estimator essentially uses these encounter probabilities (derived from trip length and visit schedule) in an H-T-like formula ￼. Similarly, if a creel samples only a subset of days, each sampled day represents other days – a sampled day’s effort can be weighted by (total days / sampled days) to estimate total effort. Design-based estimators like H-T are “expansion” estimators – they multiply observed data by raising factors to scale up to the population. These methods are unbiased by design, assuming the sampling probabilities are known and followed.
In R, the survey package excels at Horvitz-Thompson estimation. One would use svydesign() to specify the sampling design: e.g., primary sampling unit = day, with probability = (days sampled / total days); secondary stage = maybe count times, etc. Then svytotal() or svymean() can directly compute an estimate and standard error for total effort. For instance, if you have data on several sampled days with their observed effort, you can do:
d <- svydesign(data=creel_data, ids=~DayID, strata=~Stratum, fpc=~TotalDays, weights=~DayWeight)
svytotal(~Effort, design=d)
which would yield the estimated total effort and its standard error, accounting for stratification and clustering. The srvyr package provides a dplyr-like interface to do the same (e.g., creel_data %>% as_survey_design(strata=Stratum, weights=DayWeight) %>% summarize(total_effort = survey_total(Effort))). The advantage of using these packages is automated variance calculation (which can otherwise be tedious to derive for multi-stage designs).

	•	Ratio Estimation: In creel surveys, ratio estimators often appear when estimating catch-per-unit-effort (CPUE) or when using an auxiliary quantity. For effort specifically, ratio estimation might be used if there is an auxiliary measure like “traffic counts” or “number of boat trailers counted by a device” to calibrate effort. A ratio estimator would take the form $\hat{T}_{ratio} = X \cdot (\sum y_i / \sum x_i)$, where $X$ is a known total of auxiliary $x$ (e.g., total trailer count from a counter) and $y_i$ is observed effort on sampled units. While not commonly needed just for total effort (since usually we directly measure effort via counts), ratio estimation is central for combining effort and catch (catch = CPUE * effort). The R survey package supports ratio estimates through functions like svyratio(). For example, one could estimate total effort as a ratio to a known quantity if such data exist. In practice, this is more relevant to catch estimation (using effort as auxiliary), but it’s worth noting as a general survey method.
	•	Post-stratification and Calibration: Sometimes after a creel survey, one realizes certain known totals can improve estimates. For instance, you might know the total number of fishing licenses sold or total boat launches recorded, which could be used to adjust effort estimates. Post-stratification means grouping the sample and population into categories after sampling and adjusting weights so the sample aligns with known population totals in those categories. Calibration (e.g., regression estimation) can adjust the H-T estimator using auxiliary variables to reduce variance or correct bias. For creel surveys, a practical example is adjusting effort estimates if some days were missed – if effort is known to be higher on holidays, one might treat holidays as a post-stratum. The R survey package supports calibration (calibrate()) and post-stratification (postStratify()), which could be applied to creel data if, say, an independent estimate of total angler-trips from a license survey is available.
	•	Variance Estimation: An important part of any estimator is quantifying uncertainty. Design-based methods use either analytic variance formulas (for stratified multi-stage designs, these can be derived or found in literature ￼) or resampling methods. The survey package by Thomas Lumley implements Taylor series linearization for many estimators as well as jackknife and bootstrap for complex designs. For example, svytotal(~Effort, design=d, deff=TRUE) would give a standard error, and one could use svyciprop() for confidence intervals if needed. If an agency historically used replicate surveys on the same day to estimate variance (as suggested for bus-route within-day variance ￼), one could mimic that with bootstrap in R. In summary, tools like survey/srvyr make it relatively straightforward to get not just point estimates but also the precision estimates required for management use.

Implementation in R: The survey package is the primary tool for design-based analysis in R. While it is not specific to fisheries, it covers all the needed functionality: stratified sampling, clustering (multi-stage), finite population corrections, post-stratification, etc. The learning curve can be steep if one is not familiar with survey design concepts, but it ensures that estimators are mathematically correct for the design. The creelr package (Poisson Consulting) is a prototype focusing on a specific design (traditional single access point) and likely wraps around these calculations for that scenario ￼ ￼. Another related package is AnglerCreelSurveySimulation, which doesn’t do estimation from real data but can generate and simulate creel survey sampling, useful for understanding how different designs perform. In the absence of a comprehensive creel analysis package, many practitioners have relied on custom R scripts or spreadsheets, but a goal for tidycreel could be to incorporate these survey package methods under the hood to handle various designs in a user-friendly way.

Experimental and Model-Based Approaches

Beyond the classical design-based estimators, researchers have been exploring model-based and hierarchical methods to estimate fishing effort. These approaches treat the problem more like a modeling exercise: they make assumptions about the distribution of angler activity in time or space and use statistical models (often Bayesian or frequentist) to estimate effort, sometimes integrating multiple data sources. While not yet routine in management, they show promise for improving estimates or filling data gaps. Key emerging approaches include:
	•	Hierarchical Bayesian Models: Bayesian methods have been used to incorporate prior knowledge and to combine disparate data sources. For example, van Poorten et al. (2015) developed a hierarchical Bayesian model to estimate total angling effort from time-lapse camera data ￼. In that application, strategically placed cameras took photos of fishing sites at intervals, which provided incomplete observations of effort (anglers might be outside the camera’s field of view or only present between snapshots). The Bayesian model was fit to concurrent creel survey data to learn the relationship between observed camera counts and true total effort ￼ ￼. It accounted for issues like periods with zero observed anglers when in fact anglers were fishing off-camera, and even gaps in data due to camera outages ￼. By fitting a hierarchical model, they could “impute” total effort for times or locations where only camera data were available, effectively calibrating new methods (cameras) against traditional creel estimates. The hierarchical aspect often involves random effects for day-to-day variation or site-to-site differences, sharing information across the dataset. Bayesian models have also been used to model daily activity curves of anglers – for instance, a recent study (2023) on a winter fishery describes a Bayesian model of angler arrival/departure times to more accurately expand limited counts into whole-day effort ￼. By incorporating prior distributions and logically modeling how angler activity rises and falls through the day, such a model can estimate daily effort even with sparse count data.
Pros: Bayesian approaches provide a natural framework for combining data sources (e.g., creel + cameras, or creel + expert knowledge). They also yield a full posterior distribution for effort, giving a more intuitive handle on uncertainty (credible intervals). Hierarchical Bayes models can “borrow strength” across strata – e.g., if some days have no samples, the model can use information from similar days to inform the estimate. This is useful for sparse sampling scenarios. Bayesian methods can also incorporate explicit probability models for things like “angler visible to camera” vs “hidden” ￼, correcting biases that design-based methods would struggle with.
Cons: These models require more assumptions – e.g., specifying distributions for angler arrival times, or assuming a certain functional form for effort over time. If the assumptions are wrong, the estimates could be biased. They also require more computational effort (Markov Chain Monte Carlo simulation in many cases) and expertise to set up. Results can be sensitive to choice of priors or model structure. For management agencies, the lack of transparency or the complexity might be a barrier (compared to the simplicity of “count * hours”). Additionally, model-fitting can be time-consuming and might not be easily standardized across different surveys.
R Implementations: There’s no dedicated Bayesian creel package, but general tools like Stan, JAGS/BUGS, or NIMBLE are used to fit custom models. For example, one could use rstan or rstanarm to fit a hierarchical Poisson model where the observed counts are a function of true effort and some detection probability. The Fish and Wildlife Service or USGS have been developing Bayesian tools (e.g., FishStan ￼, though that one is more for population models). In published work, authors often provide pseudo-code or the model equations, which analysts could implement in R. As these methods mature, we might see template code that tidycreel could incorporate for power-users who want Bayesian estimates of effort (for instance, calibrating a new tech like drone counts against traditional data via a Bayesian model).
	•	Generalized Linear Mixed Models (GLMMs): GLMMs are another way to incorporate hierarchical structure, but using frequentist inference. A GLMM might treat daily effort counts as, say, Poisson-distributed with fixed effects for factors like weekday/weekend or weather and random effects for lake or month. One notable study applied GLMMs to combine on-site and aerial count data across dozens of lakes in Wisconsin ￼ ￼. In that work, limited samples from many lakes were modeled together: the GLMM included terms for overall seasonal patterns and lake-specific random effects, allowing estimation of total summer effort on each lake. The result was that model-based estimates of total effort were on average within ~11% of the traditional intensive survey estimates ￼. In other words, with far less sampling per lake, the GLMM was able to predict effort nearly as accurately as a full creel survey that would have been done on each lake. The trade-off was higher uncertainty, but it demonstrated the potential of spatially extensive, model-based inference to broaden coverage ￼. GLMMs have also been used to analyze patterns in creel data – for instance, to test effects of covariates (like lake size, accessibility, or demographics) on effort. They effectively can fill in missing cells in a sampling design by using a regression structure.
Pros: GLMMs can handle complex random effects (like lake and day effects) and are relatively accessible via packages like lme4 or glmmTMB. They can provide empirical Bayes predictions for unobserved units (e.g., predict effort on unsurveyed days or lakes). They also naturally accommodate overdispersion and other features if modeled (using, say, negative binomial instead of Poisson if daily counts are overdispersed). By incorporating covariates (such as weekend vs weekday, or weather conditions), they can potentially improve precision and adjust for factors that design-based methods would have to implicitly assume randomized out. For example, a GLMM could include an effect for “holiday” to adjust effort upward on those days, even if the sampling didn’t fully capture it.
Cons: As with Bayesian models, GLMMs rely on model correctness. If important predictors or random effect structures are omitted, estimates can be biased. There is also a potential for model-induced bias – unlike design-based estimators which are unbiased by design, a GLMM is only as good as its fit. Additionally, while GLMMs give you estimates and standard errors, constructing design-equivalent confidence intervals can be tricky if the model is complex. Some practitioners might be uncomfortable that model-based estimates blur the line between data and assumptions (for instance, a model might “smooth” out genuine peaks in effort). From a practical standpoint, implementing a GLMM requires statistical expertise and careful checking of residuals, fit, etc., which is more involved than a straightforward expansion formula.
R Implementations: Many R packages can fit GLMMs. lme4 is a go-to for many (using glmer() for Poisson or binomial data). glmmTMB is another powerful package that can handle zero-inflation or other distributions. The choice depends on the data – e.g., daily effort counts might be zero-inflated on some lakes (no fishing activity on a given day), which glmmTMB could model with a mixture. Once a GLMM is fit, predicting total effort for a season involves summing predictions for all days or lakes. This can be done with predict() and aggregation, or using the model’s fixed and random effects explicitly. The study mentioned above likely used a custom script; tidycreel could include wrapper functions to fit common models (e.g., a random-intercept model by waterbody for regional surveys) and output the estimated totals with uncertainties. Moreover, cross-validation can be implemented in R to test how well the model-based approach performs versus known estimates (as was done in the research). It’s worth noting that the CreelCatch R package (USGS) is aimed at modeling relationships between catch and effort across surveys ￼; while not an effort estimator per se, it indicates growing interest in model-based analysis of creel data.
	•	Model-Based (Analytic) Approaches: This is a broader category including any method that uses an assumed model of angling effort distribution rather than purely the random sampling framework. One example is using known diurnal effort curves to adjust counts. McNeish and Trial (1991) demonstrated constructing an “angler activity curve” from interview data to describe the proportion of daily effort occurring each hour, then using that to adjust aerial counts made at non-random times ￼. Essentially, if an overflight happened at a non-random time (say 10:00 AM) one can divide the observed count by the expected fraction of anglers fishing at 10:00 AM to estimate total daily effort ￼. This is a model-based correction – it assumes the activity curve is accurate and applies generally. Another example is borrowing techniques from time-series analysis or occupancy models. If we have continuous monitoring (like frequent camera counts or trail counters), one might use time-series models to interpolate and sum up effort over time. Occupancy or state-space models could be conceived for situations like, “is a given site being fished at time t or not,” though these are not common in literature for creel.
Pros: Model-based approaches can rescue situations where classical assumptions fail (e.g., nonrandom sampling times, incomplete detection). They often make more use of the data by imposing structure – for example, an activity curve approach uses the shape of angler activity to get a better estimate than a single count would. These approaches can also integrate external data like weather or seasonality in a formal way to improve estimates.
Cons: The downside is dependence on model validity. An activity curve measured on one lake or season might not transfer to another – using it blindly could misestimate effort. Also, purely model-based estimates lack the design-based guarantee of unbiasedness; they must be validated. They typically require more data upfront to fit the model (e.g., McNeish and Trial needed enough interviews per hour to establish the curve). In many cases, these are complementary to design-based methods: agencies might use them as adjustments rather than primary estimators.
R Implementations: Many of these analytical methods would use general statistical or mathematical tools in R. For example, fitting an angler activity curve might involve using nls() or mgcv (if using a spline for daily pattern). Time-series interpolation could use packages like Zoo or forecast if you treat hourly counts as a time series that needs completing. Occupancy-style models (if ever applied) could be fit with unmarked or Bayesian software. These are not out-of-the-box solutions; they’d be custom analyses. Part of tidycreel’s development might be to incorporate well-tested model-based adjustments as options (for instance, a function to apply a known hourly usage curve to a count).

Summary of Pros and Cons: Design-based methods (instantaneous/progressive counts, access surveys, etc.) are time-tested, easy to justify, and usually easier to implement with standard errors. Their downside is the need for sufficient sampling effort and inability to easily handle missing data or integrate novel data sources. Experimental model-based methods (Bayesian, GLMMs, etc.) offer flexibility and can reduce fieldwork by leveraging patterns, but they introduce model assumptions and complexity. A prudent approach in developing the tidycreel package roadmap is to prioritize validated, widely-used methods (like the various count-based designs and the standard survey estimators in R) while keeping an eye on emerging techniques that could enhance future surveys. For example, one could imagine adding a module for camera-based effort estimation using the van Poorten et al. model as agencies start employing trail cameras. Likewise, incorporating a GLMM option could help users who want to pool data from multiple lakes or years.

Conclusions

Creel surveys remain a cornerstone of fisheries management for estimating angler effort. Methods such as instantaneous and progressive counts, roving and access point designs (including the bus-route variation), and aerial counts are well-established in the fisheries toolkit, each with known trade-offs. These methods often rely on design-based statistical estimators – typically some variant of stratified sampling expansion or Horvitz-Thompson weighting – to produce unbiased effort estimates with quantifiable precision. Tools like the R survey package can implement these estimators for creel data, ensuring that estimates properly reflect the complex sampling designs used in the field. At the same time, new approaches are being explored: hierarchical Bayesian models and GLMMs show promise in integrating data sources (like cameras or multi-lake surveys) and potentially reducing the cost of surveys by exploiting patterns in angler behavior. While experimental, such approaches have yielded effort estimates close to traditional methods in case studies ￼ and have tackled scenarios (e.g., incomplete visibility or sparse sampling) that are challenging for classical designs ￼.

For the development of the tidycreel package, this review suggests a two-pronged strategy: (1) implement the core, validated methods (instantaneous count expansion, progressive/roving survey estimates, bus-route and aerial-access estimators, etc.) in a user-friendly way, leveraging R’s survey design capabilities for rigorous estimates; and (2) provide extensibility or optional modules for model-based methods, allowing advanced users to explore Bayesian or GLMM-based analyses as a complement to design-based results. By documenting assumptions, data requirements, and pros/cons of each method (as summarized above), tidycreel can guide users to choose appropriate techniques for their specific survey design and management questions. Ultimately, a combination of these methods – grounded in established survey sampling theory but augmented by modern modeling where appropriate – will provide a robust toolkit for estimating fishing effort in recreational fisheries.

Sources:
	•	Pollock, K.H., C.M. Jones, and T.L. Brown. Angler Survey Methods and Their Applications in Fisheries Management. American Fisheries Society Special Publication 25, 1994 (core reference on creel survey design and estimation).
	•	Hoenig, J.M., et al. 1993. “Scheduling Counts in the Instantaneous and Progressive Count Methods for Estimating Sportfishing Effort.” North American Journal of Fisheries Management 13: 723-736 (guidance on proper use of instantaneous vs progressive counts) ￼ ￼.
	•	Chen, S.X. and J.L. Woolcock. 1999. “A Condition for Designing Bus-Route Type Access Site Surveys to Estimate Recreational Fishing Effort.” Biometrics 55: 799-804 (theory of bus-route estimator unbiasedness) ￼ ￼.
	•	Robson, D.S. and C.M. Jones. 1989. “The Theoretical Basis of an Access Site Angler Survey Design.” Biometrics 45: 83-98 (original development of the bus-route effort estimator).
	•	Jones, C.M. et al. 1990. “Use of a Computer Model to Determine the Behavior of a New Survey Estimator of Recreational Angling.” Transactions of the American Fisheries Society 119: 41-54 (evaluation of bus-route estimator and variance) ￼.
	•	Newman, S.P. et al. 1997. “Comparison of a Stratified, Instantaneous Count Creel Survey with a Complete Mandatory Creel Census.” North American Journal of Fisheries Management 17: 321-331 (validation of instantaneous count method against a census).
	•	Dauk, P.C. 2007. Estimation in Creel Surveys Under Non-Standard Conditions. MSc Thesis, Simon Fraser University (comprehensive study of aerial-roving designs and advanced estimators) ￼ ￼.
	•	van Poorten, B.T. et al. 2015. “Imputing Recreational Angling Effort from Time-Lapse Cameras Using a Hierarchical Bayesian Model.” Fisheries Research 172: 265-273 (novel model to estimate total effort with partial camera data) ￼ ￼.
	•	Trudeau, A. et al. 2021. “Estimating Fishing Effort across the Landscape: A Spatially Extensive Approach Using Models to Integrate Multiple Data Sources.” Fisheries Research 235: 105807 (GLMM approach for multi-lake effort estimation) ￼.
	•	[R Packages: survey (Lumley 2004), srvyr, creelr (Poisson Consulting), AnglerCreelSurveySimulation (Ranney 2021)] – documentation and vignettes for these tools were referenced for implementation details ￼
